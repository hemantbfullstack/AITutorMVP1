Add natural voice output to the Tutor using ElevenLabs TTS, with a simple fallback if keys are missing.

Backend (/server, Express + TS):
1) Env:
   - ELEVENLABS_API_KEY, ELEVENLABS_VOICE_ID, ELEVENLABS_MODEL (default "eleven_multilingual_v2"), ELEVENLABS_USE_STREAMING (true/false).

2) Routes:
   - POST /api/voice/tts
     Body: { text: string, voiceId?: string, model?: string, format?: "mp3"|"wav"|"ogg" }
     Behavior:
       * Validate text (non-empty, reasonable length).
       * If ELEVENLABS_API_KEY missing → return 501 { error: "TTS not configured" }.
       * Call ElevenLabs TTS REST API (non-streaming) and return audio as binary with proper content-type (audio/mpeg default).
       * Cache short responses (e.g., hash(text) in tmp or S3 later) to save cost during dev.

   - (Optional if plan allows) GET /api/voice/tts/stream?text=...
     * Use ElevenLabs streaming endpoint (WebSocket).
     * Proxy stream to the client as chunked audio (or instruct client to connect directly).
     * If ELEVENLABS_USE_STREAMING=false → 404.

3) Security:
   - requireAuth middleware (students must be logged in).
   - Rate-limit: e.g., 30 TTS calls / 5 minutes per user.

4) Integration hook:
   - In tutor pipeline, after generating assistant text, expose an audio URL:
     * Generate via POST /api/voice/tts with the assistant text.
     * Return { audioUrl } alongside the text in the tutor response payload.

Frontend (/client, React):
1) UI:
   - On each assistant message bubble, add:
     * "Play Voice" button (▶️)
     * Toggle: "Auto-play tutor voice" (per user preference, stored locally).
     * Volume slider (controls HTMLAudioElement volume).

2) Playback:
   - When user clicks Play (or auto-play is ON), call POST /api/voice/tts with the assistant text; receive Blob → createObjectURL → play with <audio>.
   - Show small playing indicator (equalizer animation).
   - Handle errors with toast: "Voice temporarily unavailable."

3) Settings:
   - Add a simple "Voice Settings" dialog:
     * Voice selector (if we expose a few hardcoded voice IDs).
     * Auto-play ON/OFF (persist to localStorage)
     * Volume

Avatar Sync (prep for Prompt 4.5):
1) Emit simple "speaking" events:
   - Before play → set speaking=true; on ended → speaking=false.
   - Expose currentTime for basic mouth open amplitude using Web Audio API analyser (for later lip-sync).
   - No webcam access; privacy preserved.

Docs:
1) README section "Voice (ElevenLabs)":
   - How to obtain API key + voice ID.
   - Set env variables.
   - Streaming note: enable ELEVENLABS_USE_STREAMING=true if plan supports it; otherwise non-streaming endpoint works.
   - Costs reminder and caching note.
